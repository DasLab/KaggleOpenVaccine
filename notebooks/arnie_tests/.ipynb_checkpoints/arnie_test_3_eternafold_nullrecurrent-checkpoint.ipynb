{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/tunguz/arnie')\n",
    "sys.path.append('/home/tunguz')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set_style('white')\n",
    "import numpy as np\n",
    "import re\n",
    "from arnie.pfunc import pfunc\n",
    "from arnie.free_energy import free_energy\n",
    "from arnie.bpps import bpps\n",
    "from arnie.mfe import mfe\n",
    "import arnie.utils as utils\n",
    "from decimal import Decimal\n",
    "import ipynb\n",
    "import json\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split,KFold, GroupKFold,StratifiedKFold\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_WGTS = [0.3, 0.3, 0.3, 0.05, 0.05] #column weights, need to sum up to 1\n",
    "\n",
    "\n",
    "DIST_NEW = True\n",
    "DIST_NEW2 = True\n",
    "\n",
    "BBP = True\n",
    "BBP1 = True\n",
    "BBP2 = True\n",
    "BBP3 = True\n",
    "BBP4 = True\n",
    "\n",
    "BBP_TOTAL = BBP+BBP1+BBP2+BBP3+BBP4*4\n",
    "\n",
    "# This will tell us the columns we are predicting\n",
    "pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"CGCUGUCUGUACUUGUAUCAGUACACUGACGAGUCCCUAAAGGACGAAACAGCG\"\n",
    "mfe_structure = mfe(sequence, package='eternafold')\n",
    "bprna_string = write_bprna_string(mfe_structure)\n",
    "bp_matrix = bpps(sequence, package='eternafold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>bpRNA_string</th>\n",
       "      <th>structure</th>\n",
       "      <th>seq_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>CGCUGUCUGUACUUGUAUCAGUACACUGACGAGUCCCUAAAGGACG...</td>\n",
       "      <td>SSSSSSSSSSSSSHHHHHHSSSSSSBBSMMMMSSSSHHHHHSSSSM...</td>\n",
       "      <td>(((((((((((((......))))))..)....((((.....))))....</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sequence  \\\n",
       "0   0  CGCUGUCUGUACUUGUAUCAGUACACUGACGAGUCCCUAAAGGACG...   \n",
       "\n",
       "                                        bpRNA_string  \\\n",
       "0  SSSSSSSSSSSSSHHHHHHSSSSSSBBSMMMMSSSSHHHHHSSSSM...   \n",
       "\n",
       "                                           structure  seq_length  \n",
       "0  (((((((((((((......))))))..)....((((.....))))....          54  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data = [{'id': 0, 'sequence': sequence_2, 'bpRNA_string': bpRNA_string, 'structure': structure, 'seq_length': len(sequence)}])\n",
    "df.sort_values(by='seq_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_list_to_array(df):\n",
    "    \"\"\"\n",
    "    Input: dataframe of shape (x, y), containing list of length l\n",
    "    Return: np.array of shape (x, l, y)\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.transpose(\n",
    "        np.array(df.values.tolist()),\n",
    "        (0, 2, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "def calc_neighbor(d, dim, n):\n",
    "    lst_x,lst_y = np.where(d==n)\n",
    "    for c, x in enumerate(lst_x):\n",
    "        y = lst_y[c]    \n",
    "        if x+1<dim:\n",
    "            d[x+1,y] = min(d[x+1,y], n+1)\n",
    "        if y+1<dim:\n",
    "            d[x,y+1] = min(d[x,y+1], n+1)\n",
    "        if x-1>=0:\n",
    "            d[x-1,y] = min(d[x-1,y], n+1)\n",
    "        if y-1>=0:\n",
    "            d[x,y-1] = min(d[x,y-1], n+1)\n",
    "    return d\n",
    "            \n",
    "\n",
    "def get_distance_matrix_2d(Ss):\n",
    "    Ds = []\n",
    "    n = Ss.shape[0]\n",
    "    dim = Ss.shape[1]\n",
    "    for i in range(n):\n",
    "        s = Ss[i,:,:,0]\n",
    "        d = 10+np.zeros_like(s)\n",
    "        d[s==1] = 1\n",
    "        for i in range(dim):\n",
    "            d[i,i] = 0\n",
    "        for x in range(0, 9):\n",
    "            d = calc_neighbor(d, dim, x)\n",
    "        Ds.append(d)\n",
    "    Ds =  np.array(Ds) + 1\n",
    "    Ds = 1/Ds\n",
    "    Ds = Ds[:, :,:, None]\n",
    "    \n",
    "    Dss = []\n",
    "    for i in [1, 2, 4]:\n",
    "        Dss.append(Ds ** i)\n",
    "    Ds = np.stack(Dss, axis = 3)\n",
    "    return Ds[:,:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions\n",
    "def MCRMSE(y_true, y_pred):\n",
    "    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=(1))\n",
    "    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n",
    "\n",
    "\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "class MSE(losses.MeanSquaredError):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        losses.MeanSquaredError.__init__(self, *args, **kwargs)\n",
    "\n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.where(tf.math.is_nan(y_true), y_pred, y_true)\n",
    "        \n",
    "        temp = losses.MeanSquaredError.__call__(self, y_true[:, :, 0], y_pred[:, :, 0], sample_weight=None)\n",
    "        temp = tf.sqrt(temp+1e-12)\n",
    "        temp = tf.tensordot(temp,sample_weight,1)/tf.reduce_sum(sample_weight)\n",
    "        s = temp*LOSS_WGTS[0]\n",
    "#         s = tf.sqrt(temp)*LOSS_WGTS[0]\n",
    "        for i in range(1,5):\n",
    "            temp = losses.MeanSquaredError.__call__(self, y_true[:, :, i], y_pred[:, :, i], sample_weight=None)\n",
    "            temp = tf.sqrt(temp+1e-12)\n",
    "            temp = tf.tensordot(temp,sample_weight,1)/tf.reduce_sum(sample_weight)\n",
    "#             s += tf.sqrt(temp)*LOSS_WGTS[i]\n",
    "            s += (temp)*LOSS_WGTS[i]\n",
    "            \n",
    "        return s\n",
    "    \n",
    "    \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def mean_squared_error1(y_true, y_pred, sample_weight):\n",
    "    return np.sum((np.sqrt(np.mean((y_true-y_pred)**2, axis=1)))*sample_weight)/np.sum(sample_weight)\n",
    "\n",
    "def MCRMSE_NAN_sample_wgt(y_true, y_pred, sample_weight=None, loss_cap=None):\n",
    "    if loss_cap is not None:\n",
    "        y_true_adj = np.minimum(np.maximum(y_true, y_pred-loss_cap), y_pred+loss_cap)\n",
    "        return MCRMSE_NAN_sample_wgt(y_true_adj, y_pred, sample_weight=sample_weight, loss_cap=None)\n",
    "    \n",
    "    y_wgt = tf.ones_like(y_true)\n",
    "    y_true = tf.where(tf.math.is_nan(y_true), y_pred, y_true)\n",
    "\n",
    "    s = (mean_squared_error1(y_true[:, :, 0], y_pred[:, :, 0], sample_weight=sample_weight)/(tf.reduce_mean(y_wgt[:,:, 0])))*LOSS_WGTS[0]\n",
    "    for i in range(1,5):\n",
    "        s += (mean_squared_error1(y_true[:, :, i], y_pred[:, :, i], sample_weight=sample_weight)/(tf.reduce_mean(y_wgt[:,:, i])))*LOSS_WGTS[i]\n",
    "    return s\n",
    "\n",
    "def MCRMSE_NAN_sample_wgt_single(y_true, y_pred, sample_weight=None, loss_cap=None):\n",
    "    if loss_cap is not None:\n",
    "        y_true_adj = np.minimum(np.maximum(y_true, y_pred-loss_cap), y_pred+loss_cap)\n",
    "        return MCRMSE_NAN_sample_wgt_single(y_true_adj, y_pred, sample_weight=sample_weight, loss_cap=None)\n",
    "        \n",
    "    y_wgt = tf.ones_like(y_true)\n",
    "    y_true = tf.where(tf.math.is_nan(y_true), y_pred, y_true)\n",
    "\n",
    "    s = (mean_squared_error1(y_true[:, :], y_pred[:, :], sample_weight=sample_weight)/(tf.reduce_mean(y_wgt[:,:])))\n",
    "    return s\n",
    "\n",
    "\n",
    "def MCRMSE_NAN(y_true, y_pred, wgt=LOSS_WGTS, loss_cap=None):\n",
    "    return MCRMSE_NAN_sample_wgt(y_true, y_pred, sample_weight=tf.ones_like(y_true[:,0,0]), loss_cap=loss_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse inputs\n",
    "def reverse_input(train_input):\n",
    "    reverse = train_input[:, ::-1, :]\n",
    "    return reverse\n",
    "\n",
    "def reverse_BBP_3D(mat):\n",
    "    return mat[:, ::-1, ::-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/xhlulu/openvaccine-simple-gru-model\n",
    "\n",
    "def gru_layer(hidden_dim, dropout):\n",
    "    return L.Bidirectional(\n",
    "        L.GRU(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal')\n",
    "    )\n",
    "\n",
    "\n",
    "def lstm_layer(hidden_dim, dropout):\n",
    "    return L.Bidirectional(\n",
    "              L.LSTM(hidden_dim,dropout=dropout, return_sequences=True,kernel_initializer = 'orthogonal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/ragnar123/wavenet-gru-baseline\n",
    "\n",
    "def wave_block(x, filters, kernel_size, n):\n",
    "    dilation_rates = [2 ** i for i in range(n)]\n",
    "    x = tf.keras.layers.Conv1D(filters = filters, \n",
    "                               kernel_size = 1,\n",
    "                               padding = 'same')(x)\n",
    "    res_x = x\n",
    "    for dilation_rate in dilation_rates:\n",
    "        tanh_out = tf.keras.layers.Conv1D(filters = filters,\n",
    "                          kernel_size = kernel_size,\n",
    "                          padding = 'same', \n",
    "                          activation = 'tanh', \n",
    "                          dilation_rate = dilation_rate)(x)\n",
    "        sigm_out = tf.keras.layers.Conv1D(filters = filters,\n",
    "                          kernel_size = kernel_size,\n",
    "                          padding = 'same',\n",
    "                          activation = 'sigmoid', \n",
    "                          dilation_rate = dilation_rate)(x)\n",
    "        x = tf.keras.layers.Multiply()([tanh_out, sigm_out])\n",
    "        x = tf.keras.layers.Conv1D(filters = filters,\n",
    "                   kernel_size = 1,\n",
    "                   padding = 'same')(x)\n",
    "        res_x = tf.keras.layers.Add()([res_x, x])\n",
    "    return res_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main model edited from https://www.kaggle.com/mrkmakr/covid-ae-pretrain-gnn-attn-cnn\n",
    "\n",
    "def attention(x_inner, x_outer, n_factor, dropout):\n",
    "    x_Q =  L.Conv1D(n_factor, 1, activation='linear', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(x_inner)\n",
    "    x_K =  L.Conv1D(n_factor, 1, activation='linear', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(x_outer)\n",
    "    x_V =  L.Conv1D(n_factor, 1, activation='linear', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(x_outer)\n",
    "    x_KT = L.Permute((2, 1))(x_K)\n",
    "    res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) / np.sqrt(n_factor))([x_Q, x_KT])\n",
    "    att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n",
    "    att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n",
    "    return att\n",
    "\n",
    "def multi_head_attention(x, y, n_factor, n_head, dropout):\n",
    "    if n_head == 1:\n",
    "        att = attention(x, y, n_factor, dropout)\n",
    "    else:\n",
    "        n_factor_head = n_factor // n_head\n",
    "        heads = [attention(x, y, n_factor_head, dropout) for i in range(n_head)]\n",
    "        att = L.Concatenate()(heads)\n",
    "        att = L.Dense(n_factor, \n",
    "                      kernel_initializer='glorot_uniform',\n",
    "                      bias_initializer='glorot_uniform',\n",
    "                     )(att)\n",
    "    x = L.Add()([x, att])\n",
    "    x = L.LayerNormalization()(x)\n",
    "    if dropout > 0:\n",
    "        x = L.Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "def res(x, unit, kernel = 3, rate = 0.1):\n",
    "    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n",
    "    h = L.LayerNormalization()(h)\n",
    "    h = L.LeakyReLU()(h)\n",
    "    h = L.Dropout(rate)(h)\n",
    "    return L.Add()([x, h])\n",
    "\n",
    "def forward(x, unit, kernel = 3, rate = 0.1):\n",
    "    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n",
    "    h = L.LayerNormalization()(h)\n",
    "    h = L.Dropout(rate)(h)\n",
    "    h = L.LeakyReLU()(h)\n",
    "    h = res(h, unit, kernel, rate)\n",
    "    return h\n",
    "\n",
    "def adj_attn(x, adj, unit, n = 2, rate = 0.1):\n",
    "    x_a = x\n",
    "    x_as = []\n",
    "    for i in range(n):\n",
    "        x_a = forward(x_a, unit)\n",
    "        x_a = tf.matmul(adj, x_a)\n",
    "        x_as.append(x_a)\n",
    "    if n == 1:\n",
    "        x_a = x_as[0]\n",
    "    else:\n",
    "        x_a = L.Concatenate()(x_as)\n",
    "    x_a = forward(x_a, unit)\n",
    "    return x_a\n",
    "\n",
    "\n",
    "def get_base(config, dim=None):\n",
    "    node = tf.keras.Input(shape = (dim, X_node.shape[2]), name = \"node\")\n",
    "    adj = tf.keras.Input(shape = (dim, dim, As.shape[3]), name = \"adj\")\n",
    "    \n",
    "    adj_learned = L.Dense(1, \"relu\")(adj)\n",
    "    adj_all = L.Concatenate(axis = 3)([adj, adj_learned])\n",
    "        \n",
    "    xs = []\n",
    "    xs.append(node)\n",
    "    x1 = forward(node, 128*2, kernel = 3, rate = 0.1)\n",
    "    x2 = forward(x1, 64*2, kernel = 6, rate = 0.1)\n",
    "    x3 = forward(x2, 32*2, kernel = 15, rate = 0.1)\n",
    "    x4 = forward(x3, 16*2, kernel = 30, rate = 0.1)\n",
    "    x = L.Concatenate()([x1, x2, x3, x4])\n",
    "    \n",
    "    for unit in [64*2, 32*2]:\n",
    "        x_as = []\n",
    "        for i in range(adj_all.shape[3]):\n",
    "            x_a = adj_attn(x, adj_all[:, :, :, i], unit, rate = 0)\n",
    "            x_as.append(x_a)\n",
    "        x_c = forward(x, unit, kernel = 30)\n",
    "        \n",
    "        x = L.Concatenate()(x_as + [x_c])\n",
    "        x = forward(x, unit)\n",
    "        x = multi_head_attention(x, x, unit, 4, 0.0)\n",
    "        xs.append(x)\n",
    "        \n",
    "    x = L.Concatenate()(xs)\n",
    "\n",
    "    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ae_model(base, config, dim=None):\n",
    "    node = tf.keras.Input(shape = (dim, X_node.shape[2]), name = \"node\")\n",
    "    adj = tf.keras.Input(shape = (dim, dim, As.shape[3]), name = \"adj\")\n",
    "\n",
    "    x = base([L.SpatialDropout1D(0.3)(node), adj])\n",
    "    x = forward(x, 64*2, rate = 0.2)\n",
    "    p = L.Dense(X_node.shape[2], \"sigmoid\")(x)\n",
    "    \n",
    "\n",
    "    node_1 = tf.where((node>1-1e-8), node, tf.zeros_like(node))\n",
    "    node_0 = tf.where((node<1e-8), node, tf.ones_like(node))\n",
    "    node_float = tf.where((node<=1-1e-8)&(node>=1e-8), node, p) \n",
    "    \n",
    "    loss = - tf.reduce_mean(20 * node_1 * tf.math.log(p + 1e-4) + (1 - node_0) * tf.math.log(1 - p + 1e-4) - 5*(node_float-p)**2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = [node, adj], outputs = [loss])\n",
    "    \n",
    "    opt = get_optimizer()\n",
    "    model.compile(optimizer = opt, loss = lambda t, y : y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model(base, config, dim=None):\n",
    "    node = tf.keras.Input(shape = (dim, X_node.shape[2]), name = \"node\")\n",
    "    adj = tf.keras.Input(shape = (dim, dim, As.shape[3]), name = \"adj\")\n",
    "    \n",
    "    x = base([node, adj])\n",
    "    if not Diversity_type in ['forward']:\n",
    "        x = forward(x, 128*2, rate = 0.2)\n",
    "    \n",
    "    if Diversity_type == 'gru':\n",
    "        x = gru_layer(128*2, dropout=0.2)(x)\n",
    "    elif Diversity_type == 'lstm':\n",
    "        x = lstm_layer(128*2, dropout=0.2)(x)\n",
    "    elif Diversity_type == 'forward':\n",
    "        x = forward(x, 128*4, kernel=5, rate = 0.1)\n",
    "        x = forward(x, 128*4, kernel=3, rate = 0.1)\n",
    "        x = forward(x, 128*4, kernel=1, rate = 0.1)\n",
    "    elif Diversity_type == 'wave':\n",
    "        dropout = 0.1\n",
    "        x = wave_block(x, 16*2, 3, 12)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "        x = wave_block(x, 32*2, 3, 8)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "        x = wave_block(x, 64*2, 3, 4)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "        x = wave_block(x, 128*2, 3, 1)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    \n",
    "    \n",
    "    x = x[:, 1:-1,:]\n",
    "    x = L.Dense(5)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n",
    "    \n",
    "    opt = get_optimizer()\n",
    "    model.compile(optimizer = opt, loss = MSE(reduction=tf.keras.losses.Reduction.NONE))#mcrmse_loss)\n",
    "    return model\n",
    "\n",
    "def get_optimizer():\n",
    "#     sgd = tf.keras.optimizers.SGD(0.05, momentum = 0.9, nesterov=True)\n",
    "    adam = tf.optimizers.Adam()\n",
    "#     radam = tfa.optimizers.RectifiedAdam()\n",
    "#     lookahead = tfa.optimizers.Lookahead(adam, sync_period=6)\n",
    "#     swa = tfa.optimizers.SWA(adam)\n",
    "    return adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sequence\n",
    "def return_ohe(n, i):\n",
    "    tmp = [0] * n\n",
    "    tmp[i] = 1\n",
    "    return tmp\n",
    "\n",
    "def get_input(train):\n",
    "    \n",
    "    len_app = 28\n",
    "    seq_app = 'AGCUAGCUAGCUAGCUAGCUAGCUAGCU'\n",
    "    loop_app = 'SSSSMMMMIIIIBBBBHHHHEEEEXXXX'\n",
    "    stru_app = '.'*len_app\n",
    "    \n",
    "    train = train.copy()\n",
    "    train['sequence'] = train['sequence'].apply(lambda x: x+seq_app)\n",
    "    train['bpRNA_string'] = train['bpRNA_string'].apply(lambda x: x+loop_app)\n",
    "    train['structure'] = train['structure'].apply(lambda x: x+stru_app)\n",
    "    \n",
    "    mapping = {}\n",
    "    vocab = [\"A\", \"G\", \"C\", \"U\", \"s\", \"e\"]\n",
    "    for i, s in enumerate(vocab):\n",
    "        mapping[s] = return_ohe(len(vocab), i)\n",
    "    X_node = np.stack(train[\"sequence\"].apply(lambda x : list(map(lambda y : mapping[y], ['s']+list(x)+['e']))))\n",
    "\n",
    "    mapping = {}\n",
    "    vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n",
    "    for i, s in enumerate(vocab):\n",
    "        mapping[s] = return_ohe(len(vocab), i)\n",
    "    X_loop = np.stack(train[\"bpRNA_string\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n",
    "    X_loop = np.concatenate([np.zeros((X_loop.shape[0], 1, X_loop.shape[2])), X_loop, np.zeros((X_loop.shape[0], 1, X_loop.shape[2]))], axis=1)\n",
    "    \n",
    "\n",
    "    mapping = {}\n",
    "    vocab = [\".\", \"(\", \")\"]\n",
    "    for i, s in enumerate(vocab):\n",
    "        mapping[s] = return_ohe(len(vocab), i)\n",
    "    X_structure = np.stack(train[\"structure\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n",
    "    X_structure = np.concatenate([np.zeros((X_structure.shape[0], 1, X_structure.shape[2])), X_structure, np.zeros((X_structure.shape[0], 1, X_structure.shape[2]))], axis=1)\n",
    "    \n",
    "    \n",
    "    X_node = np.concatenate([X_node, X_loop], axis = 2)\n",
    "    \n",
    "    ## interaction\n",
    "    a = np.sum(X_node * (2 ** np.arange(X_node.shape[2])[None, None, :]), axis = 2)\n",
    "    vocab = sorted(set(a.flatten()))\n",
    "    #print(vocab)\n",
    "    ohes = []\n",
    "    for v in vocab:\n",
    "        ohes.append(a == v)\n",
    "    ohes = np.stack(ohes, axis = 2)\n",
    "    X_node = np.concatenate([X_node, ohes], axis = 2).astype(np.float32)\n",
    "    \n",
    "    X_node = np.concatenate([X_node[:, :(-len_app-1), :], X_node[:, -1, :][:, None,:]], axis=1)\n",
    "    #print(X_node.shape)\n",
    "    return X_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 54)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy and edited from https://www.kaggle.com/xhlulu/openvaccine-simple-gru-model\n",
    "\n",
    "token2int = {x:i for i, x in enumerate('().ACGUBEHIMSXse')}\n",
    "\n",
    "def pandas_list_to_array(df):\n",
    "    \"\"\"\n",
    "    Input: dataframe of shape (x, y), containing list of length l\n",
    "    Return: np.array of shape (x, l, y)\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.transpose(\n",
    "        np.array(df.values.tolist()),\n",
    "        (0, 2, 1)\n",
    "    )\n",
    "\n",
    "def get_pair_idx(arr, sft=0):\n",
    "    n = len(arr)\n",
    "    out = np.zeros((n))\n",
    "    l = []\n",
    "    for c, i in enumerate(arr):\n",
    "        if i == '.':\n",
    "            out[c] = c\n",
    "        elif i == '(':\n",
    "            l.append(c)\n",
    "        else:\n",
    "            temp = l.pop()\n",
    "            if sft == 0:\n",
    "                out[c] = temp\n",
    "                out[temp] = c\n",
    "            elif sft >= 1:\n",
    "                out[c] = min(temp+sft, n-1)\n",
    "                out[temp] = max(c-sft, 0)\n",
    "            elif sft <= -1:\n",
    "                out[c] = max(temp-sft, 0)\n",
    "                out[temp] = min(c+sft, n-1)\n",
    "    return out\n",
    "\n",
    "def calc_dist_to_pair(struct):\n",
    "    n = len(struct)\n",
    "    out = np.zeros((n))+10000\n",
    "    curr_dist = 10000\n",
    "    for c,i in enumerate(struct):\n",
    "        curr_dist += 1\n",
    "        if i in ['(', ')']:\n",
    "            out[c] = 1\n",
    "            curr_dist = 0\n",
    "        else:\n",
    "            out[c] = min(out[c], curr_dist)\n",
    "    curr_dist = 10000\n",
    "    for c,i in enumerate(struct[::-1]):\n",
    "        curr_dist += 1\n",
    "        if i in ['(', ')']:\n",
    "            out[n-1-c] = 0\n",
    "            curr_dist = 0\n",
    "        else:\n",
    "            out[n-1-c] = min(out[n-1-c], curr_dist)\n",
    "    return out\n",
    "\n",
    "\n",
    "def calc_dist_to_single(struct):\n",
    "    n = len(struct)\n",
    "    out = np.zeros((n))+10000\n",
    "    curr_dist = 10000\n",
    "    for c,i in enumerate(struct):\n",
    "        curr_dist += 1\n",
    "        if i == '.':\n",
    "            out[c] = 1\n",
    "            curr_dist = 0\n",
    "        else:\n",
    "            out[c] = min(out[c], curr_dist)\n",
    "    curr_dist = 10000\n",
    "    for c,i in enumerate(struct[::-1]):\n",
    "        curr_dist += 1\n",
    "        if i == '.':\n",
    "            out[n-1-c] = 0\n",
    "            curr_dist = 0\n",
    "        else:\n",
    "            out[n-1-c] = min(out[n-1-c], curr_dist)\n",
    "    return out\n",
    "\n",
    "\n",
    "def preprocess_inputs1(df, token2int, cols=['sequence', 'structure', 'bpRNA_string']):\n",
    "    return pandas_list_to_array(\n",
    "        df[cols].applymap(lambda seq: [token2int[x] for x in 's'+seq+'e'])\n",
    "    )\n",
    "def preprocess_inputs(df, token2int):\n",
    "    dict_row_idx = {}\n",
    "\n",
    "    train_inputs = preprocess_inputs1(df, token2int)\n",
    "    new = np.zeros((train_inputs.shape[0], train_inputs.shape[1], len(token2int)))\n",
    "    for layer in range(3):\n",
    "        for i in range(len(token2int)):\n",
    "            new[train_inputs[:, :, layer]==i, i]=1\n",
    "\n",
    "    if BBP_TOTAL>=1:\n",
    "        bbp =[]\n",
    "        bbp1 =[]\n",
    "        bbp2 = []\n",
    "        bbp3 = []\n",
    "        bbp4_0 = []\n",
    "        bbp4_1 = []\n",
    "        bbp4_2 = []\n",
    "        bbp4_3 = []\n",
    "\n",
    "        ids = df.id.values\n",
    "        for c, i in enumerate(ids):\n",
    "\n",
    "            probability = bp_matrix\n",
    "            if BBP:\n",
    "                bbp.append(probability.max(-1).tolist())\n",
    "            if BBP1:\n",
    "                bbp1.append((1-probability.sum(axis=1)).tolist())\n",
    "            if BBP2:\n",
    "                srt = np.sort(probability)\n",
    "                bbp2.append((srt[:,-1] - srt[:, -2]).tolist())\n",
    "            if BBP3:\n",
    "                m_lst = probability.max(axis=0)\n",
    "                argmax_lst = m_lst[np.argmax(probability, axis=0)]\n",
    "                bbp3.append((argmax_lst-m_lst).tolist())\n",
    "            if BBP4:\n",
    "                pair_idx = get_pair_idx(df.structure.values[c]).astype(int)\n",
    "                pij = probability[np.arange(len(pair_idx)),pair_idx]\n",
    "                bbp4_0.append(pij.tolist())\n",
    "                m_lst = probability.max(axis=0)\n",
    "                bbp4_1.append((m_lst-pij).tolist())\n",
    "                bbp4_2.append((m_lst[pair_idx]-pij).tolist())\n",
    "                s_lst = probability.sum(axis=0)\n",
    "                bbp4_3.append((s_lst[pair_idx]-pij).tolist())\n",
    "\n",
    "        temp = np.zeros((train_inputs.shape[0], train_inputs.shape[1]))\n",
    "        if BBP:\n",
    "            temp[:, 1:-1] = np.array(bbp)\n",
    "            dict_row_idx['BBP'] = new.shape[2]\n",
    "            new = np.concatenate([new, temp[:, :,None]], axis=2)\n",
    "        if BBP1:\n",
    "            temp[:, 1:-1] = np.array(bbp1)\n",
    "            dict_row_idx['BBP1'] = new.shape[2]\n",
    "            new = np.concatenate([new, temp[:, :,None]], axis=2)\n",
    "        if BBP2:\n",
    "            temp[:, 1:-1] = np.array(bbp2)\n",
    "            dict_row_idx['BBP2'] = new.shape[2]\n",
    "            new = np.concatenate([new, temp[:, :,None]], axis=2)\n",
    "        if BBP3:\n",
    "            temp[:, 1:-1] = np.array(bbp3)\n",
    "            dict_row_idx['BBP3'] = new.shape[2]\n",
    "            new = np.concatenate([new, temp[:, :,None]], axis=2)\n",
    "        if BBP4: \n",
    "            for cnt, b in enumerate([bbp4_0, bbp4_1, bbp4_2, bbp4_3]):\n",
    "                dict_row_idx['BBP4_%s'%cnt] = new.shape[2]\n",
    "                temp[:, 1:-1] = np.array(b)\n",
    "                new = np.concatenate([new, temp[:, :,None]], axis=2)\n",
    "            dict_row_idx['BBP4_ed'] = new.shape[2]\n",
    "\n",
    "            \n",
    "    if DIST_NEW:\n",
    "        lst_dist = []\n",
    "        lst_dist_sqrt = []\n",
    "        ids = df.id.values\n",
    "        for c, i in enumerate(ids):\n",
    "            temp_dist = calc_dist_to_pair(df['structure'].values[c])+1\n",
    "            lst_dist.append((1/temp_dist).tolist())\n",
    "            lst_dist_sqrt.append((np.sqrt(1/temp_dist)).tolist())\n",
    "        temp = np.zeros((train_inputs.shape[0], train_inputs.shape[1]))\n",
    "        temp[:, 1:-1] = np.array(lst_dist)\n",
    "        new = np.concatenate([new, temp[:, :,None]], axis=2)\n",
    "        temp = np.zeros((train_inputs.shape[0], train_inputs.shape[1]))\n",
    "        temp[:, 1:-1] = np.array(lst_dist_sqrt)\n",
    "        new = np.concatenate([new, temp[:, :,None]], axis=2)\n",
    "        \n",
    "    if DIST_NEW2:\n",
    "        lst_dist = []\n",
    "        lst_dist_sqrt = []\n",
    "        ids = df.id.values\n",
    "        for c, i in enumerate(ids):\n",
    "            temp_dist = calc_dist_to_single(df['structure'].values[c])+1\n",
    "            lst_dist.append((1/temp_dist).tolist())\n",
    "            lst_dist_sqrt.append((np.sqrt(1/temp_dist)).tolist())\n",
    "        temp = np.zeros((train_inputs.shape[0], train_inputs.shape[1]))\n",
    "        temp[:, 1:-1] = np.array(lst_dist)\n",
    "        new = np.concatenate([new, temp[:, :,None]], axis=2)\n",
    "        temp = np.zeros((train_inputs.shape[0], train_inputs.shape[1]))\n",
    "        temp[:, 1:-1] = np.array(lst_dist_sqrt)\n",
    "        new = np.concatenate([new, temp[:, :,None]], axis=2)\n",
    "    \n",
    "\n",
    "    return new[:,:,len(token2int):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structure_adj(train):\n",
    "    Ss = []\n",
    "    for i in (range(len(train))):\n",
    "        seq_length = train[\"seq_length\"].iloc[i]\n",
    "        structure = train[\"structure\"].iloc[i]\n",
    "        sequence = train[\"sequence\"].iloc[i]\n",
    "\n",
    "        cue = []\n",
    "        a_structures = {\n",
    "            (\"A\", \"U\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"C\", \"G\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"U\", \"G\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"U\", \"A\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"G\", \"C\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"G\", \"U\") : np.zeros([seq_length, seq_length]),\n",
    "        }\n",
    "        a_structure = np.zeros([seq_length, seq_length])\n",
    "        for i in range(seq_length):\n",
    "            if structure[i] == \"(\":\n",
    "                cue.append(i)\n",
    "            elif structure[i] == \")\":\n",
    "                start = cue.pop()\n",
    "                a_structures[(sequence[start], sequence[i])][start, i] = 1\n",
    "                a_structures[(sequence[i], sequence[start])][i, start] = 1\n",
    "        \n",
    "        a_strc = np.stack([a for a in a_structures.values()], axis = 2)\n",
    "        a_strc = np.sum(a_strc, axis = 2, keepdims = True)\n",
    "        Ss.append(a_strc)\n",
    "    \n",
    "    Ss = np.array(Ss)\n",
    "    new = np.zeros((Ss.shape[0], Ss.shape[1]+2, Ss.shape[2]+2, Ss.shape[3]))\n",
    "    new[:, 1:-1, 1:-1, :] = Ss\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(As):\n",
    "    idx = np.arange(As.shape[1])\n",
    "    Ds = []\n",
    "    for i in range(len(idx)):\n",
    "        d = np.abs(idx[i] - idx)\n",
    "        Ds.append(d)\n",
    "\n",
    "    Ds = np.array(Ds) + 1\n",
    "    Ds = 1/Ds\n",
    "    Ds = Ds[None, :,:]\n",
    "    Ds = np.repeat(Ds, len(As), axis = 0)\n",
    "    \n",
    "    Dss = []\n",
    "    for i in [1, 2, 4]:\n",
    "        Dss.append(Ds ** i)\n",
    "    Ds = np.stack(Dss, axis = 3)\n",
    "    return Ds\n",
    "\n",
    "\n",
    "def padding_2D(Ss):\n",
    "    new = np.zeros((Ss.shape[0], Ss.shape[1]+2, Ss.shape[2]+2))\n",
    "    new[:, 1:-1, 1:-1] = Ss\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(df_temp):\n",
    "    \n",
    "\n",
    "    X_node = get_input(df_temp).astype(np.float32)\n",
    "    X_node_new = preprocess_inputs(df_temp, token2int).astype(np.float32)\n",
    "    X_node = np.concatenate([X_node, X_node_new], axis=2)\n",
    "    del X_node_new\n",
    "\n",
    "\n",
    "    As = [bp_matrix]\n",
    "    \n",
    "    As = np.array(As)\n",
    "    As = padding_2D(As)\n",
    "    Ss = get_structure_adj(df_temp).astype(np.float32)\n",
    "    Ds = get_distance_matrix(As)\n",
    "    DDs = get_distance_matrix_2d(Ss)\n",
    "    As = np.concatenate([As[:,:,:,None],Ss, Ds, DDs], axis = 3).astype(np.float32)\n",
    "    del Ss, Ds, DDs\n",
    "    return X_node, As\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 54 is out of bounds for axis 0 with size 54",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-265480856e18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdict_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_A\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#X_node, As = dict_X[0], dict_A[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-838854b9e70f>\u001b[0m in \u001b[0;36mget_inputs\u001b[0;34m(df_temp)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mX_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mX_node_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken2int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mX_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_node_new\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mX_node_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-b63b7cf55084>\u001b[0m in \u001b[0;36mpreprocess_inputs\u001b[0;34m(df, token2int)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mBBP4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mpair_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pair_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                 \u001b[0mpij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpair_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0mbbp4_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpij\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mm_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 54 is out of bounds for axis 0 with size 54"
     ]
    }
   ],
   "source": [
    "dict_X = {}\n",
    "dict_A = {}\n",
    "for i in tqdm(df.id.values):\n",
    "    df_temp = df.loc[df.id == i]\n",
    "    dict_X[i], dict_A[i] = get_inputs(df_temp)\n",
    "    \n",
    "#X_node, As = dict_X[0], dict_A[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
